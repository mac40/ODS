{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitd7959cc20d6143b990cc00997e66c154",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Robust Neaural Network Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports and setups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from builtins import range\n",
    "from builtins import int\n",
    "from builtins import dict\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "source": [
    "## Loader class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    def __init__(self, num_samples, start=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def loadData():\n",
    "\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    MNIST_train = dset.MNIST('./Third Party/Robust-NN-Training/dataset', train=True, transform=T.ToTensor(), download=True)\n",
    "\n",
    "    MNIST_test = dset.MNIST('./Third Party/Robust-NN-Training/dataset', train=False, transform=T.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "    loader_train = DataLoader(MNIST_train, batch_size=64)\n",
    "\n",
    "    loader_test = DataLoader(MNIST_test, batch_size=64)\n",
    "\n",
    "    return loader_train, loader_test"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(800, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 1 / m.bias.numel())\n",
    "            if isinstance(m, (nn.Linear)):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 1 / m.bias.numel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 800)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "source": [
    "## Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, X, y, dtype):\n",
    "\n",
    "    N = X.shape[0]\n",
    "    X = X.repeat(1, 10, 1, 1).reshape(N * 10, 1, 28, 28)\n",
    "    X_copy = X.clone()\n",
    "    X.requires_grad = True\n",
    "\n",
    "\n",
    "    eps = 0.4\n",
    "\n",
    "    y = y.view(-1, 1).repeat(1, 10).view(-1, 1).long().cuda()\n",
    "\n",
    "    index = torch.tensor([jj for jj in range(10)] * N).view(-1, 1).cuda().long()\n",
    "\n",
    "    MaxIter_max = 11\n",
    "    step_size_max = 0.1\n",
    "\n",
    "    for i in range(MaxIter_max):\n",
    "        output = model(X)\n",
    "\n",
    "        maxLoss = (output.gather(1, index) - output.gather(1, y)).mean()\n",
    "        X_grad = torch.autograd.grad(maxLoss, X, retain_graph=True)[0]\n",
    "        X = X + X_grad.sign() * step_size_max\n",
    "\n",
    "        X.data = X_copy.data + (X.data - X_copy.data).clamp(-eps, eps)\n",
    "        X.data = X.data.clamp(0, 1)\n",
    "\n",
    "    preds = model(X)\n",
    "\n",
    "    loss = (-F.log_softmax(preds)).gather(1, y.view(-1, 1)).view(-1, 10).max(dim=1)[0].mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "source": [
    "## Training Process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader_train, loader_test, dtype):\n",
    "\n",
    "    model = ConvNet()\n",
    "    model = model.type(dtype)\n",
    "    model.train()\n",
    "\n",
    "    SCHEDULE_EPOCHS = [10, 10]\n",
    "    learning_rate = 5e-4\n",
    "\n",
    "    for num_epochs in SCHEDULE_EPOCHS:\n",
    "\t\n",
    "        print('\\nTraining %d epochs with learning rate %.7f' % (num_epochs, learning_rate))\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\t\n",
    "            print('\\nTraining epoch %d / %d ...\\n' % (epoch + 1, num_epochs))\n",
    "\n",
    "            for i, (X_, y_) in enumerate(loader_train):\n",
    "\n",
    "                X = Variable(X_.type(dtype), requires_grad=False)\n",
    "                y = Variable(y_.type(dtype), requires_grad=False)\n",
    "\n",
    "                loss = loss_function(model, X, y, dtype)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print('Batch %d done, loss = %.7f' % (i + 1, loss.item()))\n",
    "\n",
    "                    test(model, loader_test, dtype)\n",
    "\n",
    "            print('Batch %d done, loss = %.7f' % (i + 1, loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "        learning_rate *= 0.1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Training quality test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader_test, dtype):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    for X_, y_ in loader_test:\n",
    "\n",
    "        X = Variable(X_.type(dtype), requires_grad=False)\n",
    "        y = Variable(y_.type(dtype), requires_grad=False).long()\n",
    "\n",
    "        logits = model(X)\n",
    "        _, preds = logits.max(1)\n",
    "\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "\n",
    "    accuracy = float(num_correct) / num_samples * 100\n",
    "    print('\\nAccuracy = %.2f%%' % accuracy)\n",
    "    model.train()"
   ]
  },
  {
   "source": [
    "## Adversarial Attacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Projected Gradient Descent Attack"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgdAttackTest(model, loader_test, dtype):\n",
    "\n",
    "    model.eval()\n",
    "    epss = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "    MaxIter = 40\n",
    "    step_size = 1e-2\n",
    "\n",
    "    for eps in epss:\n",
    "\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "\n",
    "        for X_, y_ in loader_test:\n",
    "\n",
    "            X = Variable(X_.type(dtype), requires_grad=True)\n",
    "            X_original = Variable(X_.type(dtype), requires_grad=False)\n",
    "            y = Variable(y_.type(dtype), requires_grad=False).long()\n",
    "\n",
    "            for i in range(MaxIter):\n",
    "                logits = model(X)\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    X.data = X.data + step_size * X.grad.sign()\n",
    "                    X.data = X_original + (X.data - X_original).clamp(min=-eps, max=eps)\n",
    "                    X.data = X.data.clamp(min=0, max=1)\n",
    "                    X.grad.zero_()\n",
    "\n",
    "            X.requires_grad = False\n",
    "            X = (X * 255).long().float() / 255\n",
    "\n",
    "            logits = model(X)\n",
    "            _, preds = logits.max(1)\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        accuracy = float(num_correct) / num_samples * 100\n",
    "        print('\\nAttack using PGD with eps = %.3f, accuracy = %.2f%%' % (eps, accuracy))"
   ]
  },
  {
   "source": [
    "###  Fast Gradient Sign Method Attack"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsmAttackTest(model, loader_test, dtype):\n",
    "\n",
    "    model.eval()\n",
    "    epss = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "    for eps in epss:\n",
    "\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "\n",
    "        for X_, y_ in loader_test:\n",
    "\n",
    "            X = Variable(X_.type(dtype), requires_grad=True)\n",
    "            y = Variable(y_.type(dtype), requires_grad=False).long()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X += X.grad.sign() * eps\n",
    "                X.grad.zero_()\n",
    "\n",
    "            X.requires_grad = False\n",
    "            X = (X * 255).long().float() / 255\n",
    "\n",
    "            logits = model(X)\n",
    "            _, preds = logits.max(1)\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        accuracy = float(num_correct) / num_samples * 100\n",
    "        print('\\nAttack using FGSM with eps = %.3f, accuracy = %.2f%%' % (eps, accuracy))"
   ]
  },
  {
   "source": [
    "## Main Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_train, loader_test = loadData()\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "model = train(loader_train, loader_test, dtype)\n",
    "fname = \"./Third Party/Robust-NN-Training/model/test_model.pth\"\n",
    "torch.save(model, fname)\n",
    "print(\"Training done, model save to %s\" % fname)\n",
    "\n",
    "# fname = \"./Code/Third Party/Robust-NN-Training/model/test_model.pth\"\n",
    "# model = torch.load(fname)\n",
    "\n",
    "pgdAttackTest(model, loader_test, dtype)\n",
    "fgsmAttackTest(model, loader_test, dtype)"
   ]
  }
 ]
}